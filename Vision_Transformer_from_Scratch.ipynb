{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyPQVzr1naAglKh+4lArTchK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/bhagatpandey369/vision-transformer-from-scratch/blob/main/Vision_Transformer_from_Scratch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y2erRzaYo6dH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as T\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "image = Image.open('alihassan.png').resize((224,224))\n",
        "x = np.array(image)\n",
        "P = 16\n",
        "C = 3\n",
        "patch = x.reshape(x.shape[0]//P, P, x.shape[1]//P, P, C).swapaxes(1, 2).reshape(-1, P, P, C)\n",
        "x_p = np.reshape(patch, (-1, P*P*C))\n",
        "N = x_p.shape[0]\n",
        "N"
      ],
      "metadata": {
        "id": "Fmqd1AmUrPuL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D = 768\n",
        "B = 1\n",
        "x_p = torch.Tensor(x_p)\n",
        "x_p = x_p[None, ...]\n",
        "E = nn.Parameter(torch.randn(1, P*P*C, D))\n"
      ],
      "metadata": {
        "id": "ojK6oPNorPxs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x_p.shape, E.shape"
      ],
      "metadata": {
        "id": "HLScyhsSwT-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_embedding = torch.matmul(x_p, E)\n",
        "patch_embedding.shape"
      ],
      "metadata": {
        "id": "ZOMd8V8owiXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_token = nn.Parameter(torch.randn(1, 1, D))\n",
        "class_token.shape"
      ],
      "metadata": {
        "id": "PtNyWMUMw9GC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "patch_embeddings = torch.cat((class_token, patch_embedding), 1)\n",
        "patch_embeddings.shape"
      ],
      "metadata": {
        "id": "ae7BY5q217Q9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "E_pos = nn.Parameter(torch.randn(1, N + 1, D))\n",
        "E_pos.shape"
      ],
      "metadata": {
        "id": "UxvZb6wQ17Wb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "z0 = patch_embeddings + E_pos\n",
        "z0.shape"
      ],
      "metadata": {
        "id": "DEDa-qRq2sD_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim, key_dim=64):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.key_dim = key_dim\n",
        "    self.W = nn.Parameter(torch.randn(embedding_dim, 3 * key_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    key_dim = self.key_dim\n",
        "    qkv = torch.matmul(x, self.W)\n",
        "    q = qkv[:, :, :key_dim]\n",
        "    k = qkv[:, :, key_dim:2*key_dim]\n",
        "    v = qkv[:, :, 2*key_dim:]\n",
        "    k_T = torch.transpose(k, -2, -1)\n",
        "    dot_product = torch.matmul(q, k_T)\n",
        "    scaled_dot_products = dot_product / np.sqrt(key_dim)\n",
        "    attention_weights = F.softmax(scaled_dot_products, dim=1)\n",
        "    weight_values = torch.matmul(attention_weights, v)\n",
        "    return weight_values"
      ],
      "metadata": {
        "id": "4i8AjUWE2sHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "D_h = 64\n",
        "self_attention = SelfAttention(D, D_h)\n",
        "attention_scores = self_attention(patch_embeddings)\n",
        "attention_scores.shape"
      ],
      "metadata": {
        "id": "bfmYOLPu7FhN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadSelfAttention(nn.Module):\n",
        "  def __init__(self, embedding_dim=768, num_heads=12):\n",
        "    super().__init__()\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    assert embedding_dim % num_heads == 0, \"Embedding dimension must be divisible by number of heads\"\n",
        "    self.key_dim = embedding_dim // num_heads\n",
        "    self.attention_list = [SelfAttention(embedding_dim, self.key_dim) for _ in range(num_heads)]\n",
        "    self.multihead_attention = nn.ModuleList(self.attention_list)\n",
        "    self.W = nn.Parameter(torch.randn(num_heads * self.key_dim, embedding_dim))\n",
        "\n",
        "  def forward(self, x):\n",
        "    attention_scores = [attention(x) for attention in self.multihead_attention]\n",
        "    z = torch.cat(attention_scores, -1)\n",
        "    attention_scores = torch.matmul(z, self.W)\n",
        "    return attention_scores"
      ],
      "metadata": {
        "id": "SeXI9LiG7Fkt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_head = 12\n",
        "multi_head_attention = MultiHeadSelfAttention(D, n_head)\n",
        "attention_scores = multi_head_attention(patch_embeddings)\n",
        "attention_scores.shape"
      ],
      "metadata": {
        "id": "fAcC8nj_7Fo0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiLayerPerceptron(nn.Module):\n",
        "  def __init__(self, embedding_dim=768, hidden_dim=3072):\n",
        "    super().__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(embedding_dim, hidden_dim),\n",
        "        nn.GELU(),\n",
        "        nn.Linear(hidden_dim, embedding_dim)\n",
        "    )\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.mlp(x)\n",
        ""
      ],
      "metadata": {
        "id": "B26d-9GC7Fs-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hidden_dim = 3072\n",
        "mlp = MultiLayerPerceptron(D,hidden_dim)\n",
        "output = mlp(patch_embeddings)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "-WqFWzpsEMHl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LayerNormalization(nn.Module):\n",
        "  def __init__(self, eps:float=10**-6) -> None:\n",
        "    super().__init__()\n",
        "    self.eps = eps\n",
        "    self.alpha = nn.Parameter(torch.ones(1))\n",
        "    self.bias = nn.Parameter(torch.zeros(1))\n",
        "\n",
        "  def forward(self, x):\n",
        "    mean = x.mean(dim=-1, keepdim=True)\n",
        "    std = x.std(dim=-1, keepdim=True)\n",
        "    return self.alpha * (x - mean) / (std + self.eps) + self.bias"
      ],
      "metadata": {
        "id": "K_h7d9oMEkHt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerEncoder(nn.Module):\n",
        "  def __init__(self, embedding_dim=768, num_heads=12, hidden_dim=3072, dropout=0.1):\n",
        "    super().__init__()\n",
        "    self.MSA = MultiHeadSelfAttention(embedding_dim, num_heads)\n",
        "    self.MLP = MultiLayerPerceptron(embedding_dim, hidden_dim)\n",
        "    self.layer_norm1 = LayerNormalization(embedding_dim)\n",
        "    self.layer_norm2 = LayerNormalization(embedding_dim)\n",
        "    self.dropout1 = nn.Dropout(p=dropout)\n",
        "    self.dropout2 = nn.Dropout(p=dropout)\n",
        "    self.dropout3 = nn.Dropout(p=dropout)\n",
        "\n",
        "  def forward(self, x):\n",
        "    out_1 = self.dropout1(x)\n",
        "    out_2 = self.layer_norm1(out_1)\n",
        "    msa_out = self.MSA(out_2)\n",
        "    out_3 = self.dropout2(msa_out)\n",
        "    res_out = x + out_3\n",
        "    out_4 = self.layer_norm2(res_out)\n",
        "    mlp_out = self.MLP(out_4)\n",
        "    out_5 = self.dropout3(mlp_out)\n",
        "    output = res_out + out_5\n",
        "    return output\n"
      ],
      "metadata": {
        "id": "M0846A2pEkLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dropout_prob = 0.1\n",
        "transformer_encoder = TransformerEncoder(D, n_head, hidden_dim, dropout_prob)\n",
        "output = transformer_encoder(patch_embeddings)\n",
        "output.shape"
      ],
      "metadata": {
        "id": "WfJqh_JSJcZY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MLPHead(nn.Module):\n",
        "  def __init__(self, embedding_dim=768, num_classes=10, fine_tune=False):\n",
        "    super().__init__()\n",
        "    self.num_classes = num_classes\n",
        "    if not fine_tune:\n",
        "      self.mlp_head = nn.Sequential(\n",
        "          nn.Linear(embedding_dim, 3072),\n",
        "          nn.Tanh(),\n",
        "          nn.Linear(3072, num_classes)\n",
        "          )\n",
        "    else:\n",
        "      self.mlp_head = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.mlp_head(x)\n",
        "\n"
      ],
      "metadata": {
        "id": "FRBcjWvkJcdc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class VisionTransformer(nn.Module):\n",
        "  def __init__(self, patch_size=16, image_size=224, channel_size=3, num_layers=12, embedding_dim=768, num_heads=12, hidden_dim=3072, dropout_prob=0.1, num_classed=10, fine_tune=True):\n",
        "    super().__init__()\n",
        "    self.patch_size = patch_size\n",
        "    self.channel_size = channel_size\n",
        "    self.num_layers = num_layers\n",
        "    self.embedding_dim = embedding_dim\n",
        "    self.num_heads = num_heads\n",
        "    self.hidden_dim = hidden_dim\n",
        "    self.dropout_prob = dropout_prob\n",
        "    self.num_classes = num_classes\n",
        "    self.num_patches = int(image_size**2 / patch_size**2)\n",
        "    self.W = nn.Parameter(torch.randn(patch_size*patch_size*channel_size, embedding_dim))\n",
        "    self.pos_embedding = nn.Parameter(torch.randn(self.num_patches+1, embedding_dim))\n",
        "    self.class_token = nn.Parameter(torch.randn(1,D))\n",
        "    transformer_encoder_list = [TransformerEncoder(embedding_dim, num_heads, hidden_dim, dropout_prob) for _ in range(num_layers)]\n",
        "    self.transformer_encoder_layers = nn.Sequential(*transformer_encoder_list)\n",
        "    self.mlp_head = MLPHead(embedding_dim, num_classes)\n",
        "\n",
        "  def forward(self, x):\n",
        "    P, C = self.patch_size, self.channel_size\n",
        "    patches = x.unfold(1, C, C).unfold(2, P, P).unfold(3, P, P)\n",
        "    patches = patches.contiguous().view(patches.size(0),-1, C*P*P).float()\n",
        "    patch_embeddings = torch.matmul(patches, self.W)\n",
        "    batch_size = patch_embeddings.shape[0]\n",
        "    patch_embeddings = torch.cat((self.class_token.repeat(batch_size,1,1),patch_embeddings),1)\n",
        "    patch_embeddings = patch_embeddings + self.pos_embedding\n",
        "    transfomer_encoder_output = self.transformer_encoder_layers(patch_embeddings)\n",
        "    output_class_token = transfomer_encoder_output[:,0]\n",
        "    y = self.mlp_head(output_class_token)\n",
        "    return y\n",
        "\n"
      ],
      "metadata": {
        "id": "YgCFCL_nLqZi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_size = 224\n",
        "channel_size = 3\n",
        "num_classes = 10\n",
        "dropout_prob = 0.1\n",
        "n_layer = 12\n",
        "embedding_dim = 768\n",
        "n_head = 12\n",
        "hidden_dim = 3072\n",
        "image = Image.open('alihassan.png').resize((image_size,image_size))\n",
        "x = T.PILToTensor()(image)\n",
        "x = x[None, ...]\n",
        "patch_size = 16\n",
        "vision_transformer = VisionTransformer(patch_size, image_size, channel_size, n_layer, embedding_dim, n_head, hidden_dim, dropout_prob, num_classes)\n",
        "vit_output = vision_transformer(x)\n",
        "vit_output.shape\n"
      ],
      "metadata": {
        "id": "2FTPLXqZTt5R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vit_output"
      ],
      "metadata": {
        "id": "RXXbHVe_Tt9P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "probalilities = F.softmax(vit_output[0], dim=0)\n",
        "probalilities"
      ],
      "metadata": {
        "id": "3CSSLhBVO6F4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(torch.sum(probalilities))"
      ],
      "metadata": {
        "id": "bjQmBPEAXOfw"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}